# There will be news

The [GPT-2](https://openai.com/blog/better-language-models/) model released by [huggingface](https://github.com/huggingface/pytorch-transformers), as part of the set of pre-trained models available on [Pytorch Hub](https://pytorch.org/hub/huggingface_pytorch-transformers/), has been fine tuned using a [dataset of news](https://www.kaggle.com/rmisra/news-category-dataset) extracted from [Huffpost web](https://www.huffpost.com/).

The main goal of this project is to explore how models can output texts resembling the structure and style of news from a traditional newspaper. For that purpose, [we've developed a simple web](#) showing the latest news created by the trained model   

This trained model (1,4 GB) is not hosted in this repository as it would exceed the max limit allowed by Github. But the file `training_medium_news.ipynb` it's the Jupyter Notebook that we used for training the model, and it contains a link to a [Google Drive shared URL](https://drive.google.com/file/d/1bkgxYlj5BYXFJZ8L9fnJH2pJ8Cnx9_Ge/view?usp=sharing) from which you can download the model.  